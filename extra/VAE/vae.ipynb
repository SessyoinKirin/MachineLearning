{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd62171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = './datasets/dataset_rbmk/clear'\n",
    "\n",
    "DEVICE = torch.device( \"cpu\")\n",
    "batch_size = 100\n",
    "\n",
    "x_dim = 784\n",
    "hidden_dim = 400\n",
    "latent_dim = 20\n",
    "\n",
    "lr = 1e-3\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e335b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "dataset_path = './datasets/dataset_rbmk/clearfile'\n",
    "# 2. Ajuste as transformações\n",
    "# Nota: Como são imagens de satélite, talvez você precise de Resize ou Grayscale \n",
    "# dependendo da arquitetura do seu VAE.\n",
    "novo_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),      # Redimensiona para manter compatibilidade com x_dim=784\n",
    "    transforms.ToTensor(),            # [0, 255] -> [0, 1]\n",
    "    # transforms.Grayscale(num_output_channels=1) # Descomente se quiser forçar P&B\n",
    "])\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "# 3. Carregue o dataset completo\n",
    "full_dataset = ImageFolder(root=dataset_path, transform=novo_transform)\n",
    "\n",
    "# 4. Divisão em Treino e Teste (Já que você não tem pastas separadas nativamente)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# 5. DataLoaders permanecem quase iguais\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "# --- Verificação das dimensões ---\n",
    "# Para acessar a primeira imagem no ImageFolder dentro de um Subset:\n",
    "amostra_x, rotulo = full_dataset[0]\n",
    "canais, altura, largura = amostra_x.shape\n",
    "\n",
    "input_channels = canais\n",
    "\n",
    "print(f\"--- Info da Amostra RBMK ---\")\n",
    "print(f\"Dimensões da Imagem: {canais} canais x {altura}px x {largura}px\")\n",
    "print(f\"Total de imagens encontradas: {len(full_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d858471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.datasets import MNIST\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# mnist_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(), # [0, 255] -> [0, 1]\n",
    "# ])\n",
    "\n",
    "# kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "# train_dataset = MNIST(dataset_path, train=True, transform=mnist_transform, download=True)\n",
    "# test_dataset = MNIST(dataset_path, train=False, transform=mnist_transform, download=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "# canais_img = train_dataset[0][0].shape[0]\n",
    "# input_channels = canais_img\n",
    "# print(f'Número de canais da imagem: {canais_img}')\n",
    "\n",
    "# # Acessando o primeiro item do dataset\n",
    "# amostra_x, rotulo = train_dataset[0]\n",
    "\n",
    "# # Extraindo dimensões\n",
    "# canais = amostra_x.shape[0]\n",
    "# altura = amostra_x.shape[1]\n",
    "# largura = amostra_x.shape[2]\n",
    "\n",
    "# print(f\"--- Info da Amostra ---\")\n",
    "# print(f\"Dimensões da Imagem: {canais} canais x {altura}px x {largura}px\")\n",
    "# print(f\"Rótulo (Label): {rotulo}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e323c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# implementation of Gaussian MLP \n",
    "# '''\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, x_dim, hidden_dim, latent_dim):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.fc1 = nn.Linear(x_dim, hidden_dim) #ax + b\n",
    "#         self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.fc3_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "#         self.fc3_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "#         self.LeakyReLU = nn.LeakyReLU(0.2) # ela evita o problema de gradiente nulo, mesmo quando a entrada é negativa, o que pode ocorrer durante o treinamento.\n",
    "\n",
    "\n",
    "#         self.training = True\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         h_ = self.LeakyReLU(self.fc1(x))\n",
    "#         h_ = self.LeakyReLU(self.fc2(h_))\n",
    "#         mean = self.fc3_mean(h_)\n",
    "#         logvar = self.fc3_logvar(h_)\n",
    "#         return mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ca46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 1. Camadas Convolucionais para extração de features espaciais\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1), # Saída: 14x14\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),            # Saída: 7x7\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten() # Transforma o volume (64, 7, 7) em um vetor único\n",
    "        )\n",
    "        \n",
    "        # Precisamos calcular o tamanho da entrada após o flatten\n",
    "        # Para uma imagem 28x28 com 2 strides, chegamos a 7x7. 64 canais * 7 * 7 = 3136\n",
    "        flat_features = 64 * 7 * 7 \n",
    "        \n",
    "        # 2. Camadas Lineares para os parâmetros da Gaussiana\n",
    "        self.fc_mean = nn.Linear(flat_features, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(flat_features, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x deve ter o shape [Batch, Channels, Height, Width]\n",
    "        h_ = self.conv_layers(x)\n",
    "        \n",
    "        mean = self.fc_mean(h_)\n",
    "        logvar = self.fc_logvar(h_)\n",
    "        \n",
    "        return mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ecbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, latent_dim, hidden_dim, y_dim):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.fc3 = nn.Linear(hidden_dim, y_dim)\n",
    "\n",
    "#         self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "    \n",
    "#     def forward(self, z):\n",
    "#         h = self.LeakyReLU(self.fc1(z))\n",
    "#         h = self.LeakyReLU(self.fc2(h))\n",
    "#         y_hat = torch.sigmoid(self.fc3(h))\n",
    "#         return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec85f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # 1. Expandir o vetor latente de volta para a dimensão do último mapa de features\n",
    "        # No Encoder era (64, 7, 7), então flat_features = 3136\n",
    "        self.fc_upsample = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        \n",
    "        # 2. Camadas de Convolução Transposta (Upsampling)\n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            # Entrada: [Batch, 64, 7, 7]\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
    "            # Saída: [Batch, 32, 14, 14]\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, output_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            # Saída: [Batch, output_channels, 28, 28]\n",
    "            nn.Sigmoid() # Garante que os pixels estejam entre 0 e 1\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Transforma o vetor latente em um vetor \"largo\"\n",
    "        h = self.fc_upsample(z)\n",
    "        \n",
    "        # Faz o \"Reshape\" para o formato de volume (Batch, Channels, Height, Width)\n",
    "        # Isso é o inverso do Flatten()\n",
    "        h = h.view(-1, 64, 7, 7)\n",
    "        \n",
    "        # Passa pelas camadas de expansão espacial\n",
    "        y_hat = self.deconv_layers(h)\n",
    "        \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47514686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        dp = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(dp).to(DEVICE)\n",
    "        z = mean + dp * epsilon\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.Encoder(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        y_hat = self.Decoder(z)\n",
    "        return y_hat, mean, logvar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f27324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder(x_dim, hidden_dim, latent_dim)\n",
    "# decoder = Decoder(latent_dim, hidden_dim, x_dim)\n",
    "\n",
    "encoder = Encoder(input_channels, latent_dim)\n",
    "decoder = Decoder(latent_dim, input_channels)\n",
    "\n",
    "model = Model(encoder, decoder).to(DEVICE)\n",
    "print(\"model: \", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# BCE_loss = nn.BCELoss()\n",
    "# nn.functional.binary_cross_entropy(y_hat, x, reduction='sum')\n",
    "\n",
    "def loss_(x, y_hat, mean, logvar):\n",
    "    reproduction_loss = -torch.sum(x * torch.log(y_hat + 1e-10) + (1 - x) * torch.log(1 - y_hat + 1e-10))\n",
    "    DKL = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return reproduction_loss + DKL\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c178fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training VAE...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, (x, _) in enumerate(tqdm(train_loader)):\n",
    "        x = x.view(-1, input_channels, altura, largura).to(DEVICE) # Garantindo que x tenha o formato correto para o Encoder\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat, mean, logvar = model(x)\n",
    "\n",
    "        # Printando a média e o logvar do primeiro item do batch\n",
    "        if batch_idx == 0: # Printa apenas no primeiro batch para não inundar a tela\n",
    "            print(f\"Média (primeiros 5 valores do vetor): {mean[0][:5].detach().cpu().numpy()}\")\n",
    "            print(f\"LogVar (primeiros 5 valores do vetor): {logvar[0][:5].detach().cpu().numpy()}\")\n",
    "\n",
    "        loss = loss_(x, y_hat, mean, logvar)\n",
    "\n",
    "        overall_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(\"\\tEpoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "        #     epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "        #     100. * batch_idx / len(train_loader), overall_loss / len(train_loader.dataset)))\n",
    "        # print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / ((batch_idx+1)*batch_size))\n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / ((batch_idx+1)*batch_size))\n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95033d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ab44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x, _) in enumerate(tqdm(test_loader)):\n",
    "        # x = x.view(batch_size, x_dim).to(DEVICE)\n",
    "        x = x.view(-1, input_channels, altura, largura).to(DEVICE)\n",
    "        y_hat, _, _ = model(x)\n",
    "        \n",
    "\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede472f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_image(x, idx):\n",
    "    # 1. Debug: Printar o shape original (Batch, Canais, Altura, Largura)\n",
    "    print(f\"Shape original recebido: {x.shape}\")\n",
    "    \n",
    "    x = x.detach().cpu()\n",
    "    \n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    print(f\"Shape após permute (Batch): {x.shape}\")\n",
    "    \n",
    "    # 3. Configuração do Grid\n",
    "    grid_size = int(np.sqrt(idx))\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n",
    "    \n",
    "    # Caso idx seja 1, o subplots não retorna um array, então forçamos a lista\n",
    "    if idx == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for i in range(idx):\n",
    "        if i < len(x):\n",
    "            # Para 3 canais (RGB), imshow detecta automaticamente se o canal estiver no final\n",
    "            # Se for 1 canal após o permute, o Matplotlib ainda pode precisar de cmap='gray'\n",
    "            img_to_show = x[i].numpy()\n",
    "            \n",
    "            # Se a última dimensão for 1 (Grayscale), o imshow precisa dela 'achatada'\n",
    "            if img_to_show.shape[-1] == 1:\n",
    "                img_to_show = img_to_show.squeeze(-1)\n",
    "                axes[i].imshow(img_to_show, cmap='gray')\n",
    "            else:\n",
    "                axes[i].imshow(img_to_show)\n",
    "                \n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(x, idx=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbfea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(y_hat, idx=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        # x já vem do loader como [B, 3, H, W]\n",
    "        x = x.to(DEVICE)\n",
    "        y_hat, _, _ = model(x)\n",
    "        break\n",
    "\n",
    "n = 10 \n",
    "plt.figure(figsize=(20, 6)) # Aumentei um pouco a altura para caber melhor os títulos\n",
    "\n",
    "for i in range(n):\n",
    "    # --- Imagem Original ---\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    \n",
    "    # Prepara a imagem: move canais para o final [3, 28, 28] -> [28, 28, 3]\n",
    "    img_original = x[i].cpu().permute(1, 2, 0)\n",
    "    \n",
    "    plt.imshow(img_original)\n",
    "    plt.title(\"Original\")\n",
    "    ax.axis('off')\n",
    "\n",
    "    # --- Imagem Reconstruída ---\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    \n",
    "    # Mesmo processo para a reconstrução\n",
    "    img_reconstruida = y_hat[i].cpu().permute(1, 2, 0)\n",
    "    \n",
    "    plt.imshow(img_reconstruida)\n",
    "    plt.title(\"VAE\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
